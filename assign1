import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

dvz_handler=0.00000000000001

dataset = {'Taste': ['Salty', 'Spicy', 'Spicy', 'Spicy', 'Spicy', 'Sweet', 'Salty', 'Sweet', 'Spicy', 'Salty'],
           'Temperature': ['Hot', 'Hot', 'Hot', 'Cold', 'Hot', 'Cold', 'Cold', 'Hot', 'Cold', 'Hot'],
           'Texture': ['Soft', 'Soft', 'Hard', 'Hard', 'Hard', 'Soft', 'Soft', 'Soft', 'Soft', 'Hard'],
           'Eat': ['No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes']}

df = pd.DataFrame(dataset, columns=['Taste', 'Temperature', 'Texture', 'Eat'])

def churn_telco_preprocessor():
    df = pd.read_csv('E:/43/ML_off/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')
    df = df.drop('customerID', 1)
    features=df.keys()


    for f in features:
        if len(df[f].unique())>5:
            #print(f)
            df = binarization_by_entropy(df, f)
            print(np.unique(df[f], return_counts=True))

    # df.to_csv('Cherno_processed.csv', sep=',', encoding='utf-8')
    df.to_pickle('churn_pickled')
    return df


    #print(entropy_attribute(df, 'MonthlyCharges'))

    # df=(binarization_by_entropy(df, 'TotalCharges'))


    # print(df.TotalCharges.value_counts())
    # print("Column headings:")
    #print(list(df))

def fraud_preprocessor():
    df = pd.read_csv('E:/43/ML_off/creditcardfraud/creditcard.csv')
    df = df.drop('Time', 1)
    features=df.keys()


    for f in features:
        if len(df[f].unique())>5:
            #print(f)
            df = binarization_by_entropy(df, f)
            print(np.unique(df[f], return_counts=True))

    df.to_csv('fraud_processed.csv', sep=',', encoding='utf-8')
    return df

# print (len(df), "fffffffffffffffffff")
# data = pd.read_csv('E:/43/ML_off/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')

# print(data.columns)

# print(data.shape)

# entropy_node = 0  # Initialize Entropy
# values = df.Eat.unique()
# # print(values)
# # print(df.Eat.value_counts()['No'])
# for value in values:
#     fraction = df.Eat.value_counts()[value] / len(df.Eat)
#     entropy_node += -fraction * np.log2(fraction)
# print(entropy_node)

#print(df.keys()[-1])

def binarization_by_entropy(df, feature):
    sorted_feature_values=df[feature].sort_values().unique()
    max_entropy=-100
    best_df=df.copy(deep=True)
    #print(sorted_feature_values)
    for value in sorted_feature_values:
        temp_df=df.copy(deep=True)
        temp_df[feature] = (np.where(temp_df[feature]>=value,1,0))
        #print(temp_df[feature])
        temp_entropy=entropy_attribute(temp_df, feature)
        if temp_entropy>max_entropy and len(temp_df[feature].unique())>1:
            max_entropy=temp_entropy
            best_df=temp_df.copy(deep=True)
            #print(max_entropy)
    return best_df



# entropy ber korar jonno function lihtesi ekta..pandas e dataframe ar feature er naam pathaile oitar jonno entropy diye dibe
def entropy_attribute(data, feature):
    decision_feature=data.keys()[-1]
    decision_types=data[decision_feature].unique()

    feature_types=data[feature].unique()
    #print(len(feature_types))
    entropy = 0
    for f_t in feature_types:
        e_temp=0
        for d_t in decision_types:
            e_temp_num=len(data[feature][data[feature]==f_t][data[decision_feature]==d_t])
            e_temp_den=len(data[feature][data[feature]==f_t])
            e_temp+=-(e_temp_num/(e_temp_den+dvz_handler))*np.log2((e_temp_num/(e_temp_den))+dvz_handler)
            # print("e_temp_num", e_temp_num)
            # print("e_temp_den", e_temp_den)
            # print(e_temp)

        entropy+=(e_temp_den/len(data))*e_temp
    # print(feature_types)
    return entropy

# print(entropy_attribute(df, 'Taste'))

def entropy_dataset(data):
    decision_feature = data.keys()[-1]
    entropy = 0
    decision_types = df[decision_feature].unique()
    for d_t in decision_types:
        fraction = df[decision_feature].value_counts()[d_t] / len(df[decision_feature])
        entropy += -fraction * np.log2(fraction)
    return entropy


def max_information_gain(data):
    #Entropy_att = []
    keys = data.keys()[:-1]
    entropy_data = entropy_dataset(data)
    # print(entropy_data)
    IG = []
    for key in keys:
        IG.append(entropy_data-entropy_attribute(data,key))
        # print(IG)
    return (keys[np.argmax(IG)])


def get_subtable(df, node, val):
    return df[df[node] == val].reset_index(drop=True)


def buildTree(df, tree=None):
    decisionStub=1
    Class = df.keys()[-1]  # To make the code generic, changing target variable class name
    #print(Class)
    # Here we build our decision tree

    # Get attribute with maximum information gain
    node = max_information_gain(df)
    #print(node)

    # Get distinct value of that attribute e.g Salary is node and Low,Med and High are values
    attValue = np.unique(df[node])

    # Create an empty dictionary to create tree
    if tree is None:
        tree = {}
        tree[node] = {}

    # We make loop to construct a tree by calling this function recursively.
    # In this we check if the subset is pure and stops if it is pure.

    for value in attValue:

        subtable = get_subtable(df, node, value)
        # print(subtable, "subtable")
        clValue, counts = np.unique(subtable[list(df)[-1]], return_counts=True)
        # print(clValue, "clValue", counts, "Counts")

        if len(counts) == 1:  # Checking purity of subset
            tree[node][value] = clValue[0]
        else:
            if decisionStub==1:
                tree[node][value] = clValue[np.argmax(counts)]
            else:
                tree[node][value] = buildTree(subtable)  # Calling the function recursively

    return tree


def predict(inst, tree):

    for nodes in tree.keys():

        value = inst[nodes]
        tree = tree[nodes][value]
        prediction = 0

        if type(tree) is dict:
            prediction = predict(inst, tree)
        else:
            prediction = tree
            break;

    return prediction

def normalizer_hand(arr):
    maximum=max(arr)
    print(maximum)
    minimum=min(arr)
    return_arr ={}
    if maximum-minimum==0:
        return arr
    for i in range(len(arr)):
        return_arr[i]=arr[i]-minimum/(maximum-minimum)
    return return_arr

def resample(data, weight):
    data['weight_resamp']=weight
    data = data[(data['weight_resamp'] > 0.3)]
    data = data.drop('weight_resamp', 1)
    return data



def adaboost(data, k):
    n=len(data) #number of samples
    w = [1/n]*n #weight vector inititalize korlam
    print(normalizer_hand(w))

    h={} #save the hypotheses space
    z={} #save weights for each hypothesis
    # print(z)

    for i in range(k):
        data=resample(data, w)
        h[i]=buildTree(data)
        error=0
        for j in range(n):
            hypothesis_prediction=predict(data.iloc[j], h[i])
            if hypothesis_prediction!=data.iloc[:,-1][j]:
                error=error+w[j]
                #print(error)
        if error>0.5:
            continue
        for j in range(n):
            hypothesis_prediction = predict(data.iloc[j], h[i])
            if hypothesis_prediction==data.iloc[:,-1][j]:
                w[j]=w[j]*(error/1-error)
        normalizer_hand(w)
        # print(w)
        z[i]=w
    print(z)
    print(h)

print(df)
we=[.1,.1,.1,.1,.1,.1,.1,.1,.1,.1]
print(resample(df, we))

# df = churn_telco_preprocessor()
#df = pd.read_pickle('churn_pickled')
#train, test = train_test_split(df, test_size=0.2)


#adaboost(train,3)
#print(list(df)[-1])
# tree = buildTree(train)
# print(tree)

# print(len(test), 'test')
# print(len(train), 'train')

# print(predict(test.iloc[100],tree))


# fraud_preprocessor()