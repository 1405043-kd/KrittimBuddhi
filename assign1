import random

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

dvz_handler=0.00000000000001

dataset = {'Taste': ['Salty', 'Spicy', 'Spicy', 'Spicy', 'Spicy', 'Sweet', 'Salty', 'Sweet', 'Spicy', 'Salty'],
           'Temperature': ['Hot', 'Hot', 'Hot', 'Cold', 'Hot', 'Cold', 'Cold', 'Hot', 'Cold', 'Hot'],
           'Texture': ['Soft', 'Soft', 'Hard', 'Hard', 'Hard', 'Soft', 'Soft', 'Soft', 'Soft', 'Hard'],
           'Eat': ['No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes']}

df = pd.DataFrame(dataset, columns=['Taste', 'Temperature', 'Texture', 'Eat'])

def churn_telco_preprocessor():
    df = pd.read_csv('E:/43/ML_off/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')
    df = df.drop('customerID', 1)
    features=df.keys()


    for f in features:
        if len(df[f].unique())>5:
            #print(f)
            df = binarization_by_entropy(df, f)
            print(np.unique(df[f], return_counts=True))

    # df.to_csv('Cherno_processed.csv', sep=',', encoding='utf-8')
    df.to_pickle('churn_pickled')
    return df


    #print(entropy_attribute(df, 'MonthlyCharges'))

    # df=(binarization_by_entropy(df, 'TotalCharges'))


    # print(df.TotalCharges.value_counts())
    # print("Column headings:")
    #print(list(df))

def census_preprocessor():
    with open('E:/43/ML_off/dataset_2.txt') as f:
        # with open('F:/43/Pattern_off/eval/perceptron/trainLinearlyNonSeparable.txt') as f:
        data = []
        for line in f:
            # print(line)
            data.append([x for x in line.split(',')])

    data = pd.DataFrame(data)
    data = data.replace('\n', '', regex=True)
    data = data.replace(' ', None, regex=True)

    # s = pd.Series(data[0])
    # pd.to_numeric(s)
    # pd.to_numeric(s, errors='ignore')
    # pd.to_numeric(s, errors='coerce')
    # s.convert_objects(convert_numeric=True)
    # data[0] = data[0].astype(float).fillna(0.0)
    # data=binarization_by_entropy(data, 0)

    data[12] = data[12].astype(float).fillna(0.0)
    data = binarization_by_entropy(data, 12)


    print(data)

    # data = binarization_by_entropy(data, 2)
    # data = binarization_by_entropy(data, 4)
    # data = binarization_by_entropy(data, 10)
    # data = binarization_by_entropy(data, 10)
    # data = binarization_by_entropy(data, 11)
    # data = binarization_by_entropy(data, 12)
    # # print(binarization_by_entropy(data, 0))
    # print(np.unique(data[2], return_counts=True))
    # print(data)
    # print(data[1].dtype)


def fraud_preprocessor():
    df = pd.read_csv('E:/43/ML_off/creditcardfraud/creditcard.csv')
    df = df.drop('Time', 1)


    dff_a = df[df['Class'] == 1].reset_index(drop=True)
    dff_b = df.sample(n=10000)

    bigdata = dff_b.append(dff_a, ignore_index=True)


    # print(bigdata)
    features = bigdata.keys()

    print(features)

    for f in features:
        if len(bigdata[f].unique())>5:
            print(f)
            bigdata = binarization_by_entropy(bigdata, f)
            print(np.unique(bigdata[f], return_counts=True))

    bigdata.to_pickle('fraud_pickled')
    #
    # df.to_csv('fraud_processed.csv', sep=',', encoding='utf-8')
    return bigdata

# print (len(df), "fffffffffffffffffff")
# data = pd.read_csv('E:/43/ML_off/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')

# print(data.columns)

# print(data.shape)

# entropy_node = 0  # Initialize Entropy
# values = df.Eat.unique()
# # print(values)
# # print(df.Eat.value_counts()['No'])
# for value in values:
#     fraction = df.Eat.value_counts()[value] / len(df.Eat)
#     entropy_node += -fraction * np.log2(fraction)
# print(entropy_node)

#print(df.keys()[-1])


def binarization_by_entropy(df, feature):
    sorted_feature_values=df[feature].sort_values().unique()
    max_entropy=-100
    best_df=df.copy(deep=True)
    #print(sorted_feature_values)
    # for value in sorted_feature_values:
    for i in range(len(sorted_feature_values)-1):
        value =(sorted_feature_values[i]+sorted_feature_values[i+1])/2.0
        temp_df=df.copy(deep=True)
        temp_df[feature] = (np.where(temp_df[feature]>=value,1,0))
        #print(temp_df[feature])
        temp_entropy=entropy_attribute(temp_df, feature)
        if temp_entropy>max_entropy and len(temp_df[feature].unique())>1:
            max_entropy=temp_entropy
            best_df=temp_df.copy(deep=True)
            #print(max_entropy)
    return best_df



# entropy ber korar jonno function lihtesi ekta..pandas e dataframe ar feature er naam pathaile oitar jonno entropy diye dibe
def entropy_attribute(data, feature):
    decision_feature=data.keys()[-1]
    decision_types=data[decision_feature].unique()

    feature_types=data[feature].unique()
    #print(len(feature_types))
    entropy = 0
    for f_t in feature_types:
        e_temp=0
        for d_t in decision_types:
            e_temp_num=len(data[feature][data[feature]==f_t][data[decision_feature]==d_t])
            e_temp_den=len(data[feature][data[feature]==f_t])
            e_temp+=-(e_temp_num/(e_temp_den+dvz_handler))*np.log2((e_temp_num/(e_temp_den))+dvz_handler)
            # print("e_temp_num", e_temp_num)
            # print("e_temp_den", e_temp_den)
            # print(e_temp)

        entropy+=(e_temp_den/len(data))*e_temp
    # print(feature_types)
    return entropy

# print(entropy_attribute(df, 'Taste'))

def entropy_dataset(data):
    decision_feature = data.keys()[-1]
    entropy = 0
    decision_types = df[decision_feature].unique()
    for d_t in decision_types:
        fraction = df[decision_feature].value_counts()[d_t] / len(df[decision_feature])
        entropy += -fraction * np.log2(fraction)
    return entropy


def max_information_gain(data):
    #Entropy_att = []
    keys = data.keys()[:-1]
    entropy_data = entropy_dataset(data)
    # print(entropy_data)
    IG = []
    for key in keys:
        IG.append(entropy_data-entropy_attribute(data,key))
        # print(IG)
    return (keys[np.argmax(IG)])


def get_subtable(dfa, node, val):
    df=dfa.copy(deep=True)
    return df[df[node] == val].reset_index(drop=True)


def buildTree(df, tree=None):
    decisionStub=2
    Class = df.keys()[-1]  # To make the code generic, changing target variable class name
    #print(Class)
    # Here we build our decision tree

    # Get attribute with maximum information gain
    node = max_information_gain(df)
    #print(node)

    # Get distinct value of that attribute e.g Salary is node and Low,Med and High are values
    attValue = np.unique(df[node])

    # Create an empty dictionary to create tree
    if tree is None:
        tree = {}
        tree[node] = {}

    # We make loop to construct a tree by calling this function recursively.
    # In this we check if the subset is pure and stops if it is pure.

    for value in attValue:

        subtable = get_subtable(df, node, value)
        # print(subtable, "subtable")
        clValue, counts = np.unique(subtable[list(df)[-1]], return_counts=True)
        # print(clValue, "clValue", counts, "Counts")

        if len(counts) == 1:  # Checking purity of subset
            tree[node][value] = clValue[0]
        else:
            if decisionStub==1:
                tree[node][value] = clValue[np.argmax(counts)]
            else:
                tree[node][value] = buildTree(subtable)  # Calling the function recursively

    return tree


def predict(inst, tree):

    for nodes in tree.keys():

        value = inst[nodes]
        tree = tree[nodes][value]
        prediction = 0

        if type(tree) is dict:
            prediction = predict(inst, tree)
        else:
            prediction = tree
            break;

    return prediction

def normalizer_hand(arr):
    maximum=max(arr)
    # print(maximum, "max")
    minimum=min(arr)
    # print(minimum, "min")
    return_arr ={}
    for i in range(len(arr)):
        n = arr[i]-minimum
        # print(n, "n")
        d=maximum-minimum+dvz_handler
        # print(d, "d")
        return_arr[i]=n/d
        # print(return_arr[i], "i")
        # print(return_arr[i])
    # print(return_arr)
    return return_arr

def resample(dt, weight):
    thrdh=(max(weight)-min(weight))/2.0
    # print(thrdh)
    data=dt.copy(deep=True)
    se = pd.Series(weight)
    data.insert(loc=0, column='new_column', value=se)
    # data['weight']=se.values
    data = data[(data['new_column'] >=thrdh)]
    # print(data)
    data = data.drop('new_column', 1)
    # print(len(data))
    return data



def adaboost(data, k):
    data_a=data.copy(deep=True)

    n=len(data) #number of samples
    # print(n)
    w = [1/n]*n #weight vector inititalize korlam
    # print(w)
    # resample(data, w)
    # print(normalizer_hand(w))

    h={} #save the hypotheses space
    z={} #save weights for each hypothesis
    # print(z)

    for i in range(k):
        data_r=resample(data_a, w).copy()
        h[i]=buildTree(data_r)
        value = h[i][list(h[i].keys())[0]]
        print(value)
        if len(value)==0:
            break
        # print(h[i])
        error=0
        for j in range(n):
            # print(n)
            # print(len(data))
            hypothesis_prediction=predict(data.iloc[j], h[i])
            # print(hypothesis_prediction)
            # print(data.iloc[j][-1])
            # print(data['Churn'][j])
            # if hypothesis_prediction!=data[data.columns[-1]][j]:
            if hypothesis_prediction!=data.iloc[j][-1]:
                # print("WHAT")
                error=error+w[j]
        # print(error)
        # if error>0.5:
        #     print('damn!')
        #     continue
        for j in range(n):
            hypothesis_prediction = predict(data.iloc[j], h[i])
            # if hypothesis_prediction==data[data.columns[-1]][j]:
            if hypothesis_prediction != data.iloc[j][-1]:
                # print(error)
                w[j]=w[j]*(error/1-error)
                # print(w[j])
        # print(w)

        x=normalizer_hand(w)
        w=x.copy()

        # print(w)
        z[i]=-np.log((error+dvz_handler)/(1-error+dvz_handler))
        # print(z[i])
    return (h[np.argmax(z)])








census_preprocessor()

# # df=fraud_preprocessor()
# #
# # print(df)
#
# # df = pd.read_pickle('fraud_pickled')
#
# # print(df)
#
# # # print(df)
# # # we=[.1,.1,.1,.1,.1,.1,.1,.1,.1,.1]
# # # print(resample(df, we))
# #
# # # df = churn_telco_preprocessor()
# df = pd.read_pickle('churn_pickled')
# train, test = train_test_split(df, test_size=0.2)
# #
# #
# # tree = adaboost(train,10)
# # #print(list(df)[-1])
# tree = buildTree(train)
# print(tree)
# #
# # # print(len(test), 'test')
# # # print(len(train), 'train')
# #
# # print (tree)
# #
# # # print(predict(test.iloc[0],tree))
# #
# #
# # # fraud_preprocessor()